{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5):\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "\n",
    "# def std_agg(cnt, s1, s2): return np.sqrt((s2/cnt) - (s1/cnt)**2)\n",
    "\n",
    "# class DecisionTree():\n",
    "#     def __init__(self, x, y, n_features, f_idxs,idxs,depth=10, min_leaf=5):\n",
    "#         self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "#         self.depth = depth\n",
    "#         print(f_idxs)\n",
    "# #         print(self.depth)\n",
    "#         self.n_features = n_features\n",
    "#         self.n, self.c = len(idxs), x.shape[1]\n",
    "#         self.val = np.mean(y[idxs])\n",
    "#         self.score = float('inf')\n",
    "#         self.find_varsplit()\n",
    "        \n",
    "#     def find_varsplit(self):\n",
    "#         for i in self.f_idxs: self.find_better_split(i)\n",
    "#         if self.is_leaf: return\n",
    "#         x = self.split_col\n",
    "#         lhs = np.nonzero(x<=self.split)[0]\n",
    "#         rhs = np.nonzero(x>self.split)[0]\n",
    "#         lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "#         rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "#         self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "#         self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "\n",
    "#     def find_better_split(self, var_idx):\n",
    "#         x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "#         sort_idx = np.argsort(x)\n",
    "#         sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "#         rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "#         lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "#         for i in range(0,self.n-self.min_leaf-1):\n",
    "#             xi,yi = sort_x[i],sort_y[i]\n",
    "#             lhs_cnt += 1; rhs_cnt -= 1\n",
    "#             lhs_sum += yi; rhs_sum -= yi\n",
    "#             lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "#             if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "#                 continue\n",
    "\n",
    "#             lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "#             rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "#             curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "#             if curr_score<self.score: \n",
    "#                 self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "#     @property\n",
    "#     def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "#     @property\n",
    "#     def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "#     @property\n",
    "#     def is_leaf(self): return self.score == float('inf') or self.depth <= 0\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "#     def predict_row(self, xi):\n",
    "#         if self.is_leaf: return self.val\n",
    "#         t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "#         return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "\n",
    "# dataset = load_boston()\n",
    "# X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Implementation of the CART algorithm to train decision tree classifiers.\"\"\"\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class Node:\n",
    "#     def __init__(self, predicted_class):\n",
    "#         self.predicted_class = predicted_class\n",
    "#         self.feature_index = 0\n",
    "#         self.threshold = 0\n",
    "#         self.left = None\n",
    "#         self.right = None\n",
    "\n",
    "\n",
    "# class DecisionTreeClassifier:\n",
    "#     def __init__(self, max_depth=None):\n",
    "#         self.max_depth = max_depth\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.n_classes_ = len(set(y))\n",
    "#         self.n_features_ = X.shape[1]\n",
    "#         self.tree_ = self._grow_tree(X, y)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "#     def _best_split(self, X, y):\n",
    "#         # Num samples\n",
    "#         m = y.size\n",
    "#         if m <= 1:\n",
    "#             return None, None\n",
    "#         # category size\n",
    "#         num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "#         # worst / highest gini\n",
    "#         best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "#         best_idx, best_thr = None, None\n",
    "#         # Loop over features\n",
    "#         for idx in range(self.n_features_):\n",
    "            \n",
    "#             # select data for feature \n",
    "#             thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            \n",
    "#             # base for calculating left split gini\n",
    "#             num_left = [0] * self.n_classes_\n",
    "#             # base for calculating right split gini\n",
    "#             num_right = num_parent.copy()\n",
    "            \n",
    "#             # Loop through samples\n",
    "#             for i in range(1, m):\n",
    "                \n",
    "#                 # current label\n",
    "#                 c = classes[i - 1]\n",
    "                \n",
    "#                 # increment bases to update gini\n",
    "#                 num_left[c] += 1\n",
    "#                 num_right[c] -= 1\n",
    "#                 # Calculate gini for left and right size\n",
    "#                 gini_left = 1.0 - sum(\n",
    "#                     (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "#                 )\n",
    "#                 gini_right = 1.0 - sum(\n",
    "#                     (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "#                 )\n",
    "                \n",
    "#                 # Calculate weighted gini\n",
    "#                 gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "#                 # Contniue value is the same as previous\n",
    "#                 if thresholds[i] == thresholds[i - 1]:\n",
    "#                     continue\n",
    "#                 # update gini if we have an improvement \n",
    "#                 if gini < best_gini:\n",
    "#                     best_gini = gini\n",
    "#                     best_idx = idx\n",
    "#                     best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "#         return best_idx, best_thr\n",
    "\n",
    "#     def _grow_tree(self, X, y, depth=0):\n",
    "        \n",
    "#         # Class counts\n",
    "#         num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "#         # Predict highest probability class        \n",
    "#         predicted_class = np.argmax(num_samples_per_class)\n",
    "#         # Initialize node for predicted class\n",
    "#         node = Node(predicted_class=predicted_class)\n",
    "        \n",
    "#         if depth < self.max_depth:\n",
    "#             idx, thr = self._best_split(X, y)\n",
    "#             if idx is not None:\n",
    "#                 indices_left = X[:, idx] < thr\n",
    "#                 X_left, y_left = X[indices_left], y[indices_left]\n",
    "#                 X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "#                 node.feature_index = idx\n",
    "#                 node.threshold = thr\n",
    "#                 node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "#                 node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "#         return node\n",
    "\n",
    "#     def _predict(self, inputs):\n",
    "#         node = self.tree_\n",
    "#         while node.left:\n",
    "#             if inputs[node.feature_index] < node.threshold:\n",
    "#                 node = node.left\n",
    "#             else:\n",
    "#                 node = node.right\n",
    "#         return node.predicted_class\n",
    "\n",
    "\n",
    "# import sys\n",
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# dataset = load_iris()\n",
    "# X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "# clf = DecisionTreeClassifier(max_depth=3)\n",
    "# clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class Node:\n",
    "#     def __init__(self, prediction):\n",
    "#         self.prediction = prediction\n",
    "#         self.feature_index = 0\n",
    "#         self.threshold = 0\n",
    "#         self.left = None\n",
    "#         self.right = None\n",
    "\n",
    "\n",
    "# class DecisionTreeRegression:\n",
    "#     def __init__(self, max_depth=None, use_features = None):\n",
    "#         \"\"\"\n",
    "#         max_depth : int\n",
    "#             max depth of tree\n",
    "        \n",
    "#         use_features:\n",
    "#             features to use on each split decision\n",
    "#             supported : \n",
    "#                 'sqrt' - select int(sqrt(m)) number of features\n",
    "#                 where m is X.shape[1]\n",
    "        \n",
    "#         \"\"\"\n",
    "#         if use_features is not None:\n",
    "#             if use_features != 'sqrt':\n",
    "#                 raise ValueError(\"Supported input to use_features is 'sqrt'\")\n",
    "        \n",
    "#         self.use_features =  use_features              \n",
    "#         self.max_depth = max_depth\n",
    "#         self.nodes = []\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.n_features_ = X.shape[1]\n",
    "#         self.tree_ = self._grow_tree(X, y)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "#     def std_agg(self, count, sum_, sum_of_squared): \n",
    "#         return np.sqrt((sum_of_squared/count) - (sum_/count)**2)\n",
    "\n",
    "#     def _best_split(self, X, y):\n",
    "#         # Num samples\n",
    "#         m = y.size\n",
    "#         if m <= 1:\n",
    "#             return None, None\n",
    "        \n",
    "#         # initial variance\n",
    "#         best_var = np.inf\n",
    "#         best_idx, best_thr = None, None\n",
    "#         # Loop over features\n",
    "#         for idx in range(self.n_features_):\n",
    "#             if self.use_feautre == 'sqrt':\n",
    "#                 sample_size = int(np.sqrt(self.n_features))\n",
    "#                 condition = idx in np.random.choice(int)\n",
    "            \n",
    "            \n",
    "#             # select data for feature and sort label and feature by feature values\n",
    "#             feature = X[:, idx]\n",
    "#             sort_index = np.argsort(feature)\n",
    "#             feature = feature[sort_index]\n",
    "#             label = y[sort_index]\n",
    "            \n",
    "#             # Initial values for variance calculation\n",
    "#             rhs_cnt, rhs_sum, rhs_sum_squared = m, label.sum(), (label**2).sum()\n",
    "#             lhs_cnt, lhs_sum, lhs_sum_squared = 0, 0.0, 0.0\n",
    "            \n",
    "#             # Loop through samples\n",
    "#             for i in range(1, m):\n",
    "                \n",
    "#                 # current label\n",
    "#                 c = label[i - 1]\n",
    "                \n",
    "#                 # increment counts\n",
    "#                 lhs_cnt += 1\n",
    "#                 rhs_cnt -= 1\n",
    "#                 # increment counts\n",
    "#                 lhs_sum += c\n",
    "#                 rhs_sum -= c\n",
    "#                 lhs_sum_squared += c**2\n",
    "#                 rhs_sum_squared -= c**2\n",
    "                \n",
    "#                 # Calculate var for left and right split\n",
    "#                 var_left = self.std_agg(lhs_cnt, lhs_sum, lhs_sum_squared)\n",
    "#                 var_right = self.std_agg(rhs_cnt, rhs_sum, rhs_sum_squared)\n",
    "                                \n",
    "#                 # Calculate weighted var\n",
    "#                 w_var = var_left*lhs_cnt + var_right*rhs_cnt\n",
    "\n",
    "#                 # Update best split\n",
    "#                 if w_var<best_var:\n",
    "#                     best_var = w_var\n",
    "#                     best_idx = idx\n",
    "#                     best_thr = (feature[i] + feature[i - 1]) / 2\n",
    "            \n",
    "#         return best_idx, best_thr\n",
    "\n",
    "#     def _grow_tree(self, X, y, depth=0):\n",
    "        \n",
    "#         # Class counts\n",
    "#         # Predict highest probability class        \n",
    "#         prediction = np.mean(y)\n",
    "#         # Create node with prediction value\n",
    "#         node = Node(prediction=prediction)\n",
    "        \n",
    "#         if depth < self.max_depth:\n",
    "#             idx, thr = self._best_split(X, y)\n",
    "#             if idx is not None:\n",
    "#                 # Split on the criterion from best_split\n",
    "#                 indices_left = X[:, idx] < thr\n",
    "#                 X_left, y_left = X[indices_left], y[indices_left]\n",
    "#                 X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "#                 node.feature_index = idx\n",
    "#                 node.threshold = thr\n",
    "#                 # Recursively create branches on each node, increment depth\n",
    "#                 node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "#                 node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "#         self.nodes.append(node)\n",
    "#         return node\n",
    "\n",
    "#     def _predict(self, inputs):\n",
    "#         node = self.tree_\n",
    "#         while node.left:\n",
    "#             if inputs[node.feature_index] < node.threshold:\n",
    "#                 node = node.left\n",
    "#             else:\n",
    "#                 node = node.right\n",
    "#         return node.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.92200423638595"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "\n",
    "# dataset = load_boston()\n",
    "# X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "\n",
    "# clf = DecisionTreeRegression(max_depth=10)\n",
    "# clf.fit(X, y)\n",
    "# np.mean(np.square(y  - clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prediction):\n",
    "        self.prediction = prediction\n",
    "        self.parent = None\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.is_leaf = False\n",
    "        self.impurity = np.inf\n",
    "\n",
    "class DecisionTreeRegression:\n",
    "    def __init__(self, max_depth=None, use_features = None):\n",
    "        \"\"\"\n",
    "        max_depth : int\n",
    "            max depth of tree\n",
    "        \n",
    "        use_features:\n",
    "            features to use on each split decision\n",
    "            supported : \n",
    "                'sqrt' - select int(sqrt(m)) number of features\n",
    "                where m is X.shape[1]\n",
    "        \n",
    "        \"\"\"\n",
    "        if use_features is not None:\n",
    "            if use_features != 'sqrt':\n",
    "                raise ValueError(\"Supported input to use_features is 'sqrt'\")\n",
    "        \n",
    "        self.use_features =  use_features\n",
    "        self.max_depth = max_depth\n",
    "        self.nodes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "    def _std_dev(self, count, sum_, sum_of_squared): \n",
    "        return np.sqrt((sum_of_squared/count) - (sum_/count)**2)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        # Num samples\n",
    "        m = y.size\n",
    "        # Leaf node\n",
    "        if m <= 1:\n",
    "            return None, None, None\n",
    "        \n",
    "        # initial variance\n",
    "        best_var = np.inf\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        # Selecting random set of features to include in the split search\n",
    "        if self.use_features == 'sqrt':\n",
    "            sample_size = int(np.sqrt(self.n_features_))\n",
    "            candidates = np.random.choice(self.n_features_, sample_size, False)\n",
    "        else:\n",
    "            candidates = [i for i in range(self.n_features_)]\n",
    "        \n",
    "        # Loop over features\n",
    "        for idx in range(self.n_features_):\n",
    "            if idx not in candidates:\n",
    "                continue\n",
    "            \n",
    "            # select data for feature and sort label and feature by feature values\n",
    "            feature = X[:, idx]\n",
    "            sort_index = np.argsort(feature)\n",
    "            feature = feature[sort_index]\n",
    "            label = y[sort_index]\n",
    "            \n",
    "            # Initial values for variance calculation\n",
    "            rhs_cnt, rhs_sum, rhs_sum_squared = m, label.sum(), (label**2).sum()\n",
    "            lhs_cnt, lhs_sum, lhs_sum_squared = 0, 0.0, 0.0\n",
    "            \n",
    "            # Loop through samples\n",
    "            for i in range(1, m):\n",
    "                \n",
    "                # current label\n",
    "                c = label[i-1]\n",
    "                \n",
    "                # increment counts\n",
    "                lhs_cnt += 1\n",
    "                rhs_cnt -= 1\n",
    "                # increment sums\n",
    "                lhs_sum += c\n",
    "                rhs_sum -= c\n",
    "                lhs_sum_squared += c**2\n",
    "                rhs_sum_squared -= c**2\n",
    "                \n",
    "                # Calculate var for left and right split\n",
    "                var_left = self._std_dev(lhs_cnt, lhs_sum, lhs_sum_squared)\n",
    "                var_right = self._std_dev(rhs_cnt, rhs_sum, rhs_sum_squared)\n",
    "                                \n",
    "                # Calculate weighted var\n",
    "                w_var = var_left*lhs_cnt + var_right*rhs_cnt\n",
    "                \n",
    "                # Make sure that repeating values are kept to one side of the split\n",
    "                if feature[i] == feature[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                # Update best split\n",
    "                if w_var<best_var:\n",
    "                    best_var = w_var\n",
    "                    best_idx = idx\n",
    "                    best_thr = (feature[i] + feature[i-1]) / 2\n",
    "                    variances = (var_left, var_right)\n",
    "                    counts = (lhs_cnt, rhs_cnt)\n",
    "            \n",
    "        return best_idx, best_thr, best_var\n",
    "\n",
    "    def _grow_tree(self, X, y, parent = None, depth=0):\n",
    "        # Predict average of the group   \n",
    "        prediction = np.mean(y)\n",
    "        # Create node with prediction value\n",
    "        node = Node(prediction=prediction)\n",
    "        # Set parent on nodes after root\n",
    "        if depth > 0:\n",
    "            node.parent = parent\n",
    "        \n",
    "#         if depth < self.max_depth and node.parent.is_leaf == False:\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr, best_var = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                # Split on the criterion from best_split\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.impurity = best_var\n",
    "                # Impurity threshold\n",
    "                if depth > 0:\n",
    "                    # Check for improvement in impurity\n",
    "                    if node.impurity > node.parent.impurity:\n",
    "                        # if no improvement, set parent as leaf\n",
    "                        node.parent.is_leaf = True\n",
    "                        return node.parent\n",
    "\n",
    "                # Recursively create branches on each node, increment depth\n",
    "                node.left = self._grow_tree(X_left, y_left, node, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, node, depth + 1)\n",
    "                \n",
    "                #Link to parent\n",
    "                node.left.parent = node\n",
    "                node.right.parent = node\n",
    "                node.depth = depth\n",
    "        else:\n",
    "            node.is_leaf = True\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "    \n",
    "    \n",
    "class RandomForest():\n",
    "    def __init__(self, n_trees, tree_depth=10):\n",
    "        \"\"\"\n",
    "        n_trees : int\n",
    "            number of trees to make up the forest\n",
    "        tree_depth : int\n",
    "            depth of each tree\n",
    "        \"\"\"\n",
    "        np.random.seed(12)\n",
    "        self.n_trees = n_trees\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fitting n_trees number of trees\n",
    "        self.trees = [self._create_tree(X, y) for i in range(self.n_trees)]\n",
    "\n",
    "    def _create_tree(self, X, y):\n",
    "        # bootstrap dataset\n",
    "        idxs = np.random.choice(len(y), len(y), True)\n",
    "        # Create decision tree that uses a random subset of features at each split\n",
    "        tree = DecisionTreeRegression(max_depth = self.tree_depth, use_features = 'sqrt')\n",
    "        tree.fit(X, y)\n",
    "        return tree\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Precition as average of the individual tree predictions\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "    \n",
    "    \n",
    "class GBM():\n",
    "    def __init__(self, n_estimators, learning_rate = 0.1, tree_depth = 5):\n",
    "        \"\"\"\n",
    "        Gradient boosting algoritm\n",
    "            Implementation of gradient boosting based on mean squared error loss,\n",
    "            using decision trees.\n",
    "        \n",
    "        n_estimators : int\n",
    "            Number of trees to fit recursively\n",
    "        \n",
    "        learning rate : float\n",
    "            Size of gradient steps\n",
    "        \n",
    "        tree_depth : int\n",
    "            depth of each tree\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.tree_depth = tree_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store trees\n",
    "        self.trees = []\n",
    "        # Initial estimate\n",
    "        self.y_hat = np.array([y.mean()]*len(y))\n",
    "        self.actual = y\n",
    "        self.counter = 0\n",
    "        # fit trees\n",
    "        self._create_tree(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Base line\n",
    "        pred = self.actual.mean()\n",
    "        # Predict the residuals to update the baseline prediction\n",
    "        for tree in self.trees:\n",
    "            pred = pred - np.array(tree.predict(X)) * self.learning_rate\n",
    "        return pred\n",
    "        \n",
    "    def _create_tree(self, X):\n",
    "        # Keep track of number of trees\n",
    "        if self.counter >= self.n_estimators:\n",
    "            return\n",
    "        self.counter+= 1\n",
    "        \n",
    "        residual = self.y_hat - self.actual\n",
    "        tree = DecisionTreeRegression(max_depth = self.tree_depth, use_features = 'sqrt')\n",
    "        # Fit tree to residual\n",
    "        tree.fit(X, residual)\n",
    "        self.trees.append(tree)\n",
    "        pred = np.array(tree.predict(X))\n",
    "        # Update global prediction with residual prediction\n",
    "        self.y_hat -= pred * self.learning_rate\n",
    "        # Recursively update\n",
    "        self._create_tree(X)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9789409496376192"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "dataset = load_boston()\n",
    "X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "train_ind = np.array([i for i in range(len(X))])\n",
    "int(len(X)*0.8)\n",
    "\n",
    "clf = DecisionTreeRegression(10, 'sqrt')\n",
    "clf.fit(X, y)\n",
    "def mse(pred, actual):\n",
    "    return np.mean(np.square(pred  - actual))\n",
    "def R2(pred, actual):\n",
    "     return 1 - mse(pred, actual)/ mse(actual.mean(), actual)\n",
    "R2(clf.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9987186283944719"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForest(20, 20)\n",
    "rf.fit(X, y)\n",
    "R2(rf.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9928420955243283"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GBM(100)\n",
    "gbm.fit(X, y)\n",
    "R2(gbm.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X, y = load_boston(return_X_y=True)\n",
    "regressor = DecisionTreeRegressor(max_depth = 40)\n",
    "regressor.fit(X, y)\n",
    "R2(regressor.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features = None, depth=10, min_leaf=5):\n",
    "        self.min_leaf = min_leaf\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.find_varsplit(x, y)\n",
    "        \n",
    "    def find_varsplit(self, x, y):\n",
    "        \n",
    "        n = len(x)\n",
    "        best_score = np.inf\n",
    "        self.val = y.mean()\n",
    "        \n",
    "        for j in range(x.shape[1]):\n",
    "            feature = x[:,j]\n",
    "            sort_idx = np.argsort(feature)\n",
    "            sort_y,sort_x = y[sort_idx], feature[sort_idx]\n",
    "            rhs_cnt,rhs_sum,rhs_sum2 = len(x), sort_y.sum(), (sort_y**2).sum()\n",
    "            lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "            for i in range(0, n-self.min_leaf-1):\n",
    "                xi,yi = sort_x[i],sort_y[i]\n",
    "                lhs_cnt += 1; rhs_cnt -= 1\n",
    "                lhs_sum += yi; rhs_sum -= yi\n",
    "                lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "                if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                    continue\n",
    "\n",
    "                lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "                rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "                curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "                \n",
    "                if curr_score < best_score: \n",
    "                    self.var_idx, best_score ,self.split = j,curr_score,xi\n",
    "                \n",
    "        self.score = best_score\n",
    "        if best_score == float('inf') or self.depth <= 0:\n",
    "            print(\"Check\")\n",
    "            print(best_score)\n",
    "            print(self.depth)\n",
    "            return\n",
    "        lhs = feature <= self.split\n",
    "        rhs = feature > self.split\n",
    "#         lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "#         rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(x[lhs], y[lhs], self.n_features, \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(x[rhs], y[rhs], self.n_features, \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "\n",
    "#     def find_better_split(self, var_idx):\n",
    "#         x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "#         sort_idx = np.argsort(x)\n",
    "#         sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "#         rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "#         lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "#         for i in range(0,self.n-self.min_leaf-1):\n",
    "#             xi,yi = sort_x[i],sort_y[i]\n",
    "#             lhs_cnt += 1; rhs_cnt -= 1\n",
    "#             lhs_sum += yi; rhs_sum -= yi\n",
    "#             lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "#             if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "#                 continue\n",
    "\n",
    "#             lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "#             rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "#             curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "#             if curr_score<self.score: \n",
    "#                 self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check\n",
      "773.416171612738\n",
      "0\n",
      "Check\n",
      "inf\n",
      "0\n",
      "Check\n",
      "inf\n",
      "1\n",
      "Check\n",
      "inf\n",
      "2\n",
      "Check\n",
      "inf\n",
      "2\n",
      "Check\n",
      "inf\n",
      "1\n",
      "Check\n",
      "inf\n",
      "0\n",
      "Check\n",
      "418.10619082395374\n",
      "0\n",
      "Check\n",
      "inf\n",
      "2\n",
      "Check\n",
      "inf\n",
      "1\n",
      "Check\n",
      "inf\n",
      "0\n",
      "Check\n",
      "371.142580124162\n",
      "0\n",
      "Check\n",
      "inf\n",
      "2\n",
      "Check\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: Mean of empty slice.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Check\n",
      "inf\n",
      "0\n",
      "Check\n",
      "574.957900095367\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(X, y, depth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "        self.impurity = np.inf\n",
    "        \n",
    "    def split(self, x, y, depth):\n",
    "#         if len(y) < self.min_leaf:\n",
    "#             return self.parent\n",
    "        \n",
    "        # Value of node\n",
    "        self.value = y.mean()\n",
    "        # Initialize impurity\n",
    "        n = len(x)\n",
    "        best_impurity = np.inf\n",
    "        # Loop through features\n",
    "        for var in range(x.shape[1]):\n",
    "            # Select and sort feature and label\n",
    "            feature = x[:, var]\n",
    "            sort_ind = np.argsort(feature)\n",
    "            feature, y = feature[sort_ind], y[sort_ind]\n",
    "            \n",
    "            left_count, left_sum, left_sum_squared = 0,0.0, 0.0\n",
    "            right_count, right_sum, right_sum_squared = n, np.sum(y), np.sum(y**2)\n",
    "            \n",
    "            for i in range(1,n):\n",
    "                label = y[i-1]\n",
    "                \n",
    "                left_count += 1\n",
    "                right_count -= 1\n",
    "                \n",
    "                left_sum += label\n",
    "                right_sum -= label\n",
    "                \n",
    "                left_sum_squared += label**2\n",
    "                right_sum_squared -= label**2\n",
    "                \n",
    "                left_var = left_sum_squared / left_count - (left_sum/left_count)**2\n",
    "                right_var = right_sum_squared / right_count - (right_sum/right_count)**2\n",
    "                impurity = left_count * left_var + right_count * right_var\n",
    "                \n",
    "                if feature[i] == feature[i-1]:\n",
    "                    continue\n",
    "                \n",
    "                if impurity < best_impurity:\n",
    "                    best_impurity = impurity\n",
    "                    self.split_variable = var\n",
    "                    self.threshold = (feature[i] + feature[i-1])/2\n",
    "                    self.impurity = best_impurity\n",
    "        \n",
    "        left_index = x[:, self.split_variable] <= self.threshold\n",
    "        left_x, left_y = x[left_index], y[left_index]\n",
    "        right_x, right_y = x[~left_index], y[~left_index]\n",
    "        self.left = Node()\n",
    "        self.left.split(left_x, left_y)\n",
    "        self.right = Node()\n",
    "        self.right.split(right_x, right_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "                    \n",
    "                    \n",
    "class DecisionTree:\n",
    "    def __init__(self, depth, n_features = None):\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        \n",
    "    def _build(self, x, y):\n",
    "        node = Node()\n",
    "        node.split(x, y)\n",
    "        \n",
    "        feature = x[:, node.split_variable]\n",
    "        left_index = feature <= node.threshold\n",
    "        \n",
    "        left_x, left_y = x[left_index], y[left_index]\n",
    "        right_x, right_y = x[~left_index], y[~left_index]        \n",
    "    \n",
    "        left_node = Node()\n",
    "        left_node.split(left_x, left_y)\n",
    "        \n",
    "        right_node = Node()\n",
    "        right_node.split(right_x, right_y)\n",
    "        \n",
    "        node.left = left_node\n",
    "        node.right = right_node\n",
    "        \n",
    "        \n",
    "                            \n",
    "    \n",
    "class RootNode(Node):\n",
    "    def __init__(self, X, y):\n",
    "#         self.idx = [i for i in range(len(idx))]\n",
    "        self.impurity = np.inf\n",
    "        \n",
    "\n",
    "\n",
    "# class DecisionTree:\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "\n",
    "#     def split(self):\n",
    "#         idx = self.find_best_split_variable(x, y)\n",
    "#         X = self.x[:, idx]\n",
    "#         value = self.find_best_split_value(x)\n",
    "\n",
    "#     def gini_impurity(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(y-tree.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features = None, f_idx = None, depth=10, min_leaf=5):\n",
    "        self.x, self.y = x, y\n",
    "        self.min_leaf = min_leaf\n",
    "        self.f_idx = f_idx\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = x.shape[0], x.shape[1]\n",
    "        self.val = np.mean(y)\n",
    "        self.score = float('inf')\n",
    "        self._split()\n",
    "    \n",
    "    def _calc_std(self, count, sum_, squared_sum):\n",
    "        return np.sqrt((squared_sum/count) - (sum_/count)**2)\n",
    "        \n",
    "    def _split(self):\n",
    "        for var_idx in range(self.x.shape[1]):\n",
    "            x, y = self.x[:,var_idx], self.y\n",
    "            sort_idx = np.argsort(x)\n",
    "            sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "            rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "            lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "            for i in range(0,self.n-1):\n",
    "                xi,yi = sort_x[i],sort_y[i]\n",
    "                lhs_cnt += 1; rhs_cnt -= 1\n",
    "                lhs_sum += yi; rhs_sum -= yi\n",
    "                lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "                if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                    continue\n",
    "\n",
    "                lhs_std = self._calc_std(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "                rhs_std = self._calc_std(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "                curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "                if curr_score<self.score: \n",
    "                    self.var_idx,self.score,self.split = var_idx,curr_score, (xi+sort_x[i+1])/2\n",
    "            \n",
    "        if self.is_leaf: return\n",
    "        x = self.collect_split_feature\n",
    "        lhs = np.nonzero(x<=self.split)[0]\n",
    "        rhs = np.nonzero(x>self.split)[0]\n",
    "        if self.n_features:\n",
    "            lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "            rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x[lhs], self.y[lhs], self.n_features, \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x[rhs], self.y[rhs], self.n_features, \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)        \n",
    "    \n",
    "    @property\n",
    "    def collect_split_feature(self): return self.x[:,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') \\\n",
    "            or self.depth <= 0 \\\n",
    "            or self.x.shape[0] <= self.min_leaf\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([self._predict_row(xi) for xi in x])\n",
    "\n",
    "    def _predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t._predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in sqrt\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "dataset = load_boston()\n",
    "X, y = dataset.data, dataset.target\n",
    "clf = DecisionTree(X, y, depth = 20, min_leaf = 0)\n",
    "np.mean(np.square(y-clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8039362740449915"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(y  - regressor.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.375254237288136"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.right.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.tree_.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTAT'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature_names[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.right.parent.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.right.right.parent.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.right.parent.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.right.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.501430190592042"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(y-clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.nodes[0].feature_index\n",
    "clf.nodes[0].threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Node at 0x1fbe69fe518>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.nodes[4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5, 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
